{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STEP3-Sentiment_Analysis_use.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1Y4il5yGbEli",
        "OBQ1pR3udfOj",
        "ubiKKJQBgt_R",
        "D5rLpjfuDr_h",
        "1AKtyQUkVCTj"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDArBIfeR1vA"
      },
      "source": [
        "#  Sentiement Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSIHt96nyTD5"
      },
      "source": [
        "Context\r\n",
        "\r\n",
        "This is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the twitter api . The tweets have been annotated (0 = negative, 4 = positive) and they can be used to detect sentiment .\r\n",
        "Content\r\n",
        "\r\n",
        "It contains the following 6 fields:\r\n",
        "\r\n",
        "  - target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\r\n",
        "\r\n",
        "  - ids: The id of the tweet ( 2087)\r\n",
        "\r\n",
        "  - date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\r\n",
        "\r\n",
        "  - flag: The query (lyx). If there is no query, then this value is NO_QUERY.\r\n",
        "\r\n",
        "  - user: the user that tweeted (robotickilldozr)\r\n",
        "\r\n",
        "  - text: the text of the tweet (Lyx is cool)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xINnEuWjyT56",
        "outputId": "525f425d-2d55-4048-d459-1f509cf0cc1f"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.text import Text\r\n",
        "from nltk.stem.lancaster import LancasterStemmer\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "import nltk\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07WGLC5y0ISN"
      },
      "source": [
        "# I- Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT-aKhXQ0QZ6",
        "outputId": "5e58f688-9f15-4661-97d1-ed047cc582bf"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive/')\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nQIaN1b66TZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7975116-8a27-462d-dfda-d3e71f488016"
      },
      "source": [
        "import requests\r\n",
        "import json\r\n",
        "api_key='q1HJoEmRMdypie_E6lYwiih0hNfuSatSTQXH6Bh9fnltm30JX4XO1TSCd6NARRVszrs4lMXb7N3i6LWB_UIBEinyG_ah-0expG6GUnzTdUss2gvNQRcJ7fJhK5g2YHYx'\r\n",
        "headers = {'Authorization': 'Bearer %s' % api_key}\r\n",
        "\r\n",
        "\r\n",
        "url = \"https://api.yelp.com/v3/businesses/FEVQpbOPOwAPNIgO7D3xxw/reviews\"\r\n",
        "req = requests.get(url, headers=headers)\r\n",
        "print('the status code is {}'.format(req.status_code))\r\n",
        "\r\n",
        "\r\n",
        "data=json.loads(req.text)\r\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the status code is 200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'possible_languages': ['fr', 'en', 'zh', 'pt', 'de', 'it', 'sv', 'ja', 'es'],\n",
              " 'reviews': [{'id': 'qOiuCTz_Gzgmose1GHaAlg',\n",
              "   'rating': 5,\n",
              "   'text': \"Truth be told if it was up to me I'd be giving 4/5 stars, we did order recently and our cheese fries weren't delivered, couldn't speak with anyone from...\",\n",
              "   'time_created': '2020-12-19 07:52:35',\n",
              "   'url': 'https://www.yelp.com/biz/shake-shack-new-york-2?adjust_creative=KzZVmcfbYqCMY9omoha5NA&hrid=qOiuCTz_Gzgmose1GHaAlg&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_reviews&utm_source=KzZVmcfbYqCMY9omoha5NA',\n",
              "   'user': {'id': 'FRKnLBnlFiasr1Dc8oOIGQ',\n",
              "    'image_url': 'https://s3-media3.fl.yelpcdn.com/photo/1wm0yjWSw92j_ZsUFZBGzQ/o.jpg',\n",
              "    'name': 'Sarah G.',\n",
              "    'profile_url': 'https://www.yelp.com/user_details?userid=FRKnLBnlFiasr1Dc8oOIGQ'}},\n",
              "  {'id': '7d56A_ObMPHHyywcfhnrUw',\n",
              "   'rating': 5,\n",
              "   'text': 'Happened to be in the city today and not too far from here so I had to stop by and pick up cheese fries (I love those crinkle cuts!) and a shake. Yum. Tbh,...',\n",
              "   'time_created': '2021-02-25 19:56:35',\n",
              "   'url': 'https://www.yelp.com/biz/shake-shack-new-york-2?adjust_creative=KzZVmcfbYqCMY9omoha5NA&hrid=7d56A_ObMPHHyywcfhnrUw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_reviews&utm_source=KzZVmcfbYqCMY9omoha5NA',\n",
              "   'user': {'id': '77MIRZBZqbQKoonun8qWjQ',\n",
              "    'image_url': 'https://s3-media2.fl.yelpcdn.com/photo/ptL063kjqRUDQ6pny96iqg/o.jpg',\n",
              "    'name': 'Ipsita D.',\n",
              "    'profile_url': 'https://www.yelp.com/user_details?userid=77MIRZBZqbQKoonun8qWjQ'}},\n",
              "  {'id': 'SYykUNBT2dJznMV5PpRnhg',\n",
              "   'rating': 5,\n",
              "   'text': 'Before I moved to NY, my first ever trip to the city was in 2005 - this was when Shake Shack had this one lone original location, and it was an event to go...',\n",
              "   'time_created': '2020-12-12 15:19:49',\n",
              "   'url': 'https://www.yelp.com/biz/shake-shack-new-york-2?adjust_creative=KzZVmcfbYqCMY9omoha5NA&hrid=SYykUNBT2dJznMV5PpRnhg&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_reviews&utm_source=KzZVmcfbYqCMY9omoha5NA',\n",
              "   'user': {'id': 'yw2cJk_SfGZlcoZKEUevxw',\n",
              "    'image_url': 'https://s3-media1.fl.yelpcdn.com/photo/EH2WTffzgKs72GHqa4kn6A/o.jpg',\n",
              "    'name': 'Thomas V.',\n",
              "    'profile_url': 'https://www.yelp.com/user_details?userid=yw2cJk_SfGZlcoZKEUevxw'}}],\n",
              " 'total': 5609}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9X3u0wsDixFe",
        "outputId": "759e4155-e47d-4442-bf2e-8015432914f3"
      },
      "source": [
        "text  = data[\"reviews\"][0][\"text\"]\r\n",
        "print(text)\r\n",
        "print(type(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truth be told if it was up to me I'd be giving 4/5 stars, we did order recently and our cheese fries weren't delivered, couldn't speak with anyone from...\n",
            "<class 'str'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5PSzFEeYLUc"
      },
      "source": [
        "## B - Clean data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjeLr3RKYTRp"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pG9tcnCpYO9t",
        "outputId": "565cef43-36fd-46b1-83e4-7444a2a55c71"
      },
      "source": [
        "from time import time\r\n",
        "from nltk.tokenize import TweetTokenizer \r\n",
        "start_time = time()\r\n",
        "tokenizer  = TweetTokenizer(reduce_len = True)\r\n",
        "#convert dataframe to list\r\n",
        "data = []\r\n",
        "\r\n",
        "#Create a X training matrix and a Y label matrix \r\n",
        "X_text_list = [text]\r\n",
        "\r\n",
        "\r\n",
        "# Convert label value \"4: positive or 1: negative\" to 0 or 1 : binary values\r\n",
        "# 1 : postive comment\r\n",
        "# 0 : negative comment\r\n",
        "for text in X_text_list:\r\n",
        "    data.append((tokenizer.tokenize(text)))\r\n",
        "print(\"Computation Time\", time() - start_time)\r\n",
        "data[:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computation Time 0.0016748905181884766\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Truth',\n",
              "  'be',\n",
              "  'told',\n",
              "  'if',\n",
              "  'it',\n",
              "  'was',\n",
              "  'up',\n",
              "  'to',\n",
              "  'me',\n",
              "  \"I'd\",\n",
              "  'be',\n",
              "  'giving',\n",
              "  '4/5',\n",
              "  'stars',\n",
              "  ',',\n",
              "  'we',\n",
              "  'did',\n",
              "  'order',\n",
              "  'recently',\n",
              "  'and',\n",
              "  'our',\n",
              "  'cheese',\n",
              "  'fries',\n",
              "  \"weren't\",\n",
              "  'delivered',\n",
              "  ',',\n",
              "  \"couldn't\",\n",
              "  'speak',\n",
              "  'with',\n",
              "  'anyone',\n",
              "  'from',\n",
              "  '...']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo7NL0tyai63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffe1d3d5-f32c-49ca-cb01-09436e3a5861"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y4il5yGbEli"
      },
      "source": [
        "### Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKFnP_KybHSV",
        "outputId": "68c6c344-3e1e-44d9-ff34-0344508dcf8d"
      },
      "source": [
        "from nltk.tag  import pos_tag\r\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\r\n",
        "\r\n",
        "def lemmatize_token(tokens):\r\n",
        "  \"\"\"\r\n",
        "  take a token in input return its lemmatize version\r\n",
        "  \"\"\"\r\n",
        "  lemmatizer = WordNetLemmatizer()\r\n",
        "  lemmatized_token = []\r\n",
        "  for word, tag in pos_tag(tokens):\r\n",
        "    if (tag.startswith('NN')):\r\n",
        "      pos = 'n'\r\n",
        "    elif (tag.startswith('VB')):\r\n",
        "      pos = 'v'\r\n",
        "    else:\r\n",
        "      pos = 'a'\r\n",
        "    lemmatized_token.append(lemmatizer.lemmatize(word, pos))\r\n",
        "  return lemmatized_token\r\n",
        "\r\n",
        "print(lemmatize_token(data[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Truth', 'be', 'tell', 'if', 'it', 'be', 'up', 'to', 'me', \"I'd\", 'be', 'give', '4/5', 'star', ',', 'we', 'do', 'order', 'recently', 'and', 'our', 'cheese', 'fry', \"weren't\", 'deliver', ',', \"couldn't\", 'speak', 'with', 'anyone', 'from', '...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oXMzQ1iclCk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBQ1pR3udfOj"
      },
      "source": [
        "### Remove stopword\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV1RRqCIdhz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d929a2fb-afa4-4138-cf2b-170fa9dcff4d"
      },
      "source": [
        "import re, string\r\n",
        "from nltk.corpus import stopwords\r\n",
        "STOPWORD = stopwords.words('english')\r\n",
        "\r\n",
        "def cleaner(token):\r\n",
        "    if token == 'u':\r\n",
        "      return 'you'\r\n",
        "    if token == 'r':\r\n",
        "        return 'are'\r\n",
        "    if token == 'some1':\r\n",
        "        return 'someone'\r\n",
        "    if token == 'yrs':\r\n",
        "        return 'years'\r\n",
        "    if token == 'hrs':\r\n",
        "        return 'hours'\r\n",
        "    if token == 'mins':\r\n",
        "        return 'minutes'\r\n",
        "    if token == 'secs':\r\n",
        "        return 'seconds'\r\n",
        "    if token == 'pls' or token == 'plz':\r\n",
        "        return 'please'\r\n",
        "    if token == '2morow':\r\n",
        "        return 'tomorrow'\r\n",
        "    if token == '2day':\r\n",
        "        return 'today'\r\n",
        "    if token == '4got' or token == '4gotten':\r\n",
        "        return 'forget'\r\n",
        "    if token == 'amp' or token == 'quot' or token == 'lt' or token == 'gt' or token == '½25':\r\n",
        "        return ''\r\n",
        "    return token\r\n",
        "\r\n",
        "\r\n",
        "def remove_noise(tokens):\r\n",
        "\r\n",
        "  cleaned_tokens = []\r\n",
        "\r\n",
        "  for token, tag in pos_tag(tokens):\r\n",
        "      # Eliminating the token if it is a link\r\n",
        "      token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\r\n",
        "                      '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\r\n",
        "      # Eliminating the token if it is a mention\r\n",
        "      token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\r\n",
        "\r\n",
        "      if tag.startswith(\"NN\"):\r\n",
        "          pos = 'n'\r\n",
        "      elif tag.startswith('VB'):\r\n",
        "          pos = 'v'\r\n",
        "      else:\r\n",
        "          pos = 'a'\r\n",
        "\r\n",
        "      lemmatizer = WordNetLemmatizer()\r\n",
        "      token = lemmatizer.lemmatize(token, pos)\r\n",
        "\r\n",
        "      cleaned_token = cleaner(token.lower())\r\n",
        "      \r\n",
        "      # Eliminating the token if its length is less than 3, if it is a punctuation or if it is a stopword\r\n",
        "      if cleaned_token not in string.punctuation and len(cleaned_token) > 2 and cleaned_token not in STOPWORD:\r\n",
        "          cleaned_tokens.append(cleaned_token)\r\n",
        "          \r\n",
        "  return cleaned_tokens\r\n",
        "\r\n",
        "\r\n",
        "print(remove_noise(data[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['truth', 'tell', \"i'd\", 'give', '4/5', 'star', 'order', 'recently', 'cheese', 'fry', 'deliver', 'speak', 'anyone', '...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G3duh1OeZqi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53b3dee5-da6d-490d-ca97-aeaf0e292fed"
      },
      "source": [
        "start_time = time()\r\n",
        "\r\n",
        "\r\n",
        "# we have to define a function that transforms our data into the required input structure\r\n",
        "def list_to_dict(cleaned_tokens):\r\n",
        "    return dict([token, True] for token in cleaned_tokens)\r\n",
        "\r\n",
        "cleaned_tokens_list = []\r\n",
        "\r\n",
        "# Removing noise from all the data\r\n",
        "for tokens in data:\r\n",
        "    cleaned_tokens_list.append((remove_noise(tokens)))\r\n",
        "\r\n",
        "print('Removed Noise, CPU Time:', time() - start_time)\r\n",
        "start_time = time()\r\n",
        "\r\n",
        "final_data = []\r\n",
        "\r\n",
        "# Transforming the data to fit the input structure of the Naive Bayesian classifier\r\n",
        "for tokens in cleaned_tokens_list:\r\n",
        "    final_data.append((list_to_dict(tokens)))\r\n",
        "    \r\n",
        "print('Data Prepared for model, CPU Time:', time() - start_time)\r\n",
        "\r\n",
        "# Previewing our final (tokenized, cleaned and lemmatized) data list\r\n",
        "print(final_data[:5])\r\n",
        "print(cleaned_tokens_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Removed Noise, CPU Time: 0.007508277893066406\n",
            "Data Prepared for model, CPU Time: 0.00015878677368164062\n",
            "[{'truth': True, 'tell': True, \"i'd\": True, 'give': True, '4/5': True, 'star': True, 'order': True, 'recently': True, 'cheese': True, 'fry': True, 'deliver': True, 'speak': True, 'anyone': True, '...': True}]\n",
            "[['truth', 'tell', \"i'd\", 'give', '4/5', 'star', 'order', 'recently', 'cheese', 'fry', 'deliver', 'speak', 'anyone', '...']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubiKKJQBgt_R"
      },
      "source": [
        "### Word embedding using GloVe\r\n",
        "first I downloaded the glovefile text on kaggle and put it on my GoogleDrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoJQ6zNjgyYA"
      },
      "source": [
        "# Defining a handy function in order to load a given glove file\r\n",
        "import numpy as np\r\n",
        "def read_glove_vecs(glove_file):\r\n",
        "    with open(glove_file, 'r', encoding=\"utf8\") as f:\r\n",
        "        words = set()\r\n",
        "        word_to_vec_map = {}\r\n",
        "        for line in f:\r\n",
        "            line = line.strip().split()\r\n",
        "            curr_word = line[0]\r\n",
        "            words.add(curr_word)\r\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\r\n",
        "        \r\n",
        "        i = 1\r\n",
        "        words_to_index = {}\r\n",
        "        index_to_words = {}\r\n",
        "        for w in sorted(words):\r\n",
        "            words_to_index[w] = i\r\n",
        "            index_to_words[i] = w\r\n",
        "            i = i + 1\r\n",
        "    return words_to_index, index_to_words, word_to_vec_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtWd3d_h0oRl"
      },
      "source": [
        "# Loading the 50-dimensional GloVe embeddings\r\n",
        "# This method will return three dictionaries:\r\n",
        "# * word_to_index: a dictionary mapping from words to their indices in the vocabulary\r\n",
        "# * index_to_word: dictionary mapping from indices to their corresponding words in the vocabulary\r\n",
        "# * word_to_vec_map: dictionary mapping words to their GloVe vector representation\r\n",
        "# Note that there are 400,001 words, with the valid indices ranging from 0 to 400,000\r\n",
        "\r\n",
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('/content/gdrive/MyDrive/Etudes/Natural_Language_Processing/SentimentAnalysis/glove.6B.100d.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRMO--1K18qR"
      },
      "source": [
        "print(\"word to index\", word_to_index['hello'])\r\n",
        "print(\"word to vec map\", word_to_vec_map['hello'])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anCW9ne82K7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5a03ac8-311e-4db9-d747-d2b90d052667"
      },
      "source": [
        "start_time = time()\r\n",
        "\r\n",
        "unks = []\r\n",
        "UNKS = []\r\n",
        "\r\n",
        "# This function will act as a \"last resort\" in order to try and find the word\r\n",
        "# in the words embedding layer. It will basically eliminate contiguously occuring\r\n",
        "# instances of a similar character\r\n",
        "def cleared(word):\r\n",
        "    res = \"\"\r\n",
        "    prev = None\r\n",
        "    for char in word:\r\n",
        "        if char == prev: continue\r\n",
        "        prev = char\r\n",
        "        res += char\r\n",
        "    return res\r\n",
        "\r\n",
        "\r\n",
        "def sentence_to_indices(sentence_words, word_to_index, max_len, i):\r\n",
        "    global X, Y\r\n",
        "    sentence_indices = []\r\n",
        "    for j, w in enumerate(sentence_words):\r\n",
        "        try:\r\n",
        "            index = word_to_index[w]\r\n",
        "        except:\r\n",
        "            UNKS.append(w)\r\n",
        "            w = cleared(w)\r\n",
        "            try:\r\n",
        "                index = word_to_index[w]\r\n",
        "            except:\r\n",
        "                index = word_to_index['unk']\r\n",
        "                unks.append(w)\r\n",
        "        X[i, j] = index\r\n",
        "\r\n",
        "        \r\n",
        "# Here we will utilize the already computed 'cleaned_tokens_list' variable\r\n",
        "   \r\n",
        "print('Removed Noise, CPU Time:', time() - start_time)\r\n",
        "start_time = time()\r\n",
        "\r\n",
        "list_len = [len(i) for i in cleaned_tokens_list]\r\n",
        "max_len = max(list_len)\r\n",
        "print('max_len:', max_len)\r\n",
        "\r\n",
        "X = np.zeros((len(cleaned_tokens_list), max_len))\r\n",
        "Y = np.zeros((len(cleaned_tokens_list), ))\r\n",
        "\r\n",
        "for i, tokens in enumerate(cleaned_tokens_list):\r\n",
        "    print(\"i\",i)\r\n",
        "    print('tokens: ', tokens)\r\n",
        "    #tokens, label = tk_lb\r\n",
        "    sentence_to_indices(tokens, word_to_index, max_len, i)\r\n",
        "    #Y[i] = label\r\n",
        "print(X)\r\n",
        "print('Data Prepared for model, CPU Time:', time() - start_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Removed Noise, CPU Time: 0.000225067138671875\n",
            "max_len: 14\n",
            "i 0\n",
            "tokens:  ['truth', 'tell', \"i'd\", 'give', '4/5', 'star', 'order', 'recently', 'cheese', 'fry', 'deliver', 'speak', 'anyone', '...']\n",
            "[[366230. 355345. 372306. 162051.  27552. 341678. 271173. 302802.  97865.\n",
            "  154596. 120116. 338956.  57187.    868.]]\n",
            "Data Prepared for model, CPU Time: 0.0019001960754394531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlrywr6nsZ8M",
        "outputId": "34d7ce11-1853-4129-911e-370dcb4432bc"
      },
      "source": [
        "X[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([366230., 355345., 372306., 162051.,  27552., 341678., 271173.,\n",
              "       302802.,  97865., 154596., 120116., 338956.,  57187.,    868.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "un7s5gOSs0J9",
        "outputId": "e371dc5f-9b11-4b99-851c-b27508bbdaa7"
      },
      "source": [
        "api_key='q1HJoEmRMdypie_E6lYwiih0hNfuSatSTQXH6Bh9fnltm30JX4XO1TSCd6NARRVszrs4lMXb7N3i6LWB_UIBEinyG_ah-0expG6GUnzTdUss2gvNQRcJ7fJhK5g2YHYx'\r\n",
        "headers = {'Authorization': 'Bearer %s' % api_key}\r\n",
        "\r\n",
        "\r\n",
        "url = \"https://api.yelp.com/v3/businesses/FEVQpbOPOwAPNIgO7D3xxw/reviews\"\r\n",
        "req = requests.get(url, headers=headers)\r\n",
        "print('the status code is {}'.format(req.status_code))\r\n",
        "\r\n",
        "\r\n",
        "data=json.loads(req.text)\r\n",
        "text_to_predict =data[\"reviews\"][0][\"text\"]\r\n",
        "vect = X[0]\r\n",
        "from tensorflow import keras\r\n",
        "model = keras.models.load_model(\"/content/gdrive/MyDrive/Etudes/Natural_Language_Processing/SentimentAnalysis/save/\")\r\n",
        "model.predict(vect)\r\n",
        "# I don't understand this output, considering the Y matrix I was supposed to have an array of shape (1,1)  + the final dense layer has a size of 1 ! "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the status code is 200\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 29) for input KerasTensor(type_spec=TensorSpec(shape=(None, 29), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 1).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.3516037 ],\n",
              "       [0.37039268],\n",
              "       [0.4029287 ],\n",
              "       [0.44840705],\n",
              "       [0.3950941 ],\n",
              "       [0.47120404],\n",
              "       [0.42520157],\n",
              "       [0.37630567],\n",
              "       [0.37837526],\n",
              "       [0.35819   ],\n",
              "       [0.50418544],\n",
              "       [0.4415063 ],\n",
              "       [0.32905757],\n",
              "       [0.38366202]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    }
  ]
}